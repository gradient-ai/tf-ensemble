{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Set up the env"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Seed environment \n",
    "seed_value = 1 # seed value \n",
    "\n",
    "# Set`PYTHONHASHSEED` environment variable at a fixed value \n",
    "import os os.environ['PYTHONHASHSEED']=str(seed_value) \n",
    "\n",
    "# Set the `python` built-in pseudo-random generator at a fixed value \n",
    "import random \n",
    "random.seed = seed_value \n",
    "\n",
    "# Set the `numpy` pseudo-random generator at a fixed value \n",
    "import numpy as np \n",
    "np.random.seed = seed_value \n",
    "\n",
    "# Set the `tensorflow` pseudo-random generator at a fixed value import tensorflow as tf tf.seed = seed_value"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set data configurations and get the classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# set configs \n",
    "base_path = './natural_images' \n",
    "target_size = (224,224,3) \n",
    "\n",
    "# define shape for all images # get classes classes = os.listdir(base_path) print(classes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot sample images "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "f, axes = plt.subplots(2, 4, sharex=True, sharey=True, figsize = (16,7))\n",
    "\n",
    "for ax, label in zip(axes.ravel(), classes):\n",
    "    img = np.random.choice(os.listdir(os.path.join(base_path, label)))\n",
    "    img = cv2.imread(os.path.join(base_path, label, img))\n",
    "    img = cv2.resize(img, target_size[:2])\n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGRA2RGB))\n",
    "    ax.set_title(label)\n",
    "    ax.axis(False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the images"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=20,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   width_shift_range = 0.2,\n",
    "                                   height_shift_range = 0.2,\n",
    "                                   vertical_flip = True,\n",
    "                                   validation_split=0.25)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_gen = datagen.flow_from_directory(base_path,\n",
    "                                               target_size=target_size[:2],\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='categorical',\n",
    "                                               subset='training')\n",
    "val_gen =  datagen.flow_from_directory(base_path,\n",
    "                                               target_size=target_size[:2],\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='categorical',\n",
    "                                               subset='validation',\n",
    "                                               shuffle=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build and train a custom model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Build model \n",
    "input = Input(shape= target_size) \n",
    "\n",
    "x = Conv2D(filters=32, kernel_size=(3,3), activation='relu')(input) \n",
    "x = MaxPool2D(2,2)(x) \n",
    "\n",
    "x = Conv2D(filters=64, kernel_size=(3,3), activation='relu')(x) \n",
    "x = MaxPool2D(2,2)(x) \n",
    "\n",
    "x = Conv2D(filters=128, kernel_size=(3,3), activation='relu')(x) \n",
    "x = MaxPool2D(2,2)(x) \n",
    "\n",
    "x = Conv2D(filters=256, kernel_size=(3,3), activation='relu')(x) \n",
    "x = MaxPool2D(2,2)(x) \n",
    "\n",
    "x = Dropout(0.25)(x) \n",
    "x = Flatten()(x) \n",
    "x = Dense(units=128, activation='relu')(x) \n",
    "x = Dense(units=64, activation='relu')(x) \n",
    "output = Dense(units=8, activation='softmax')(x) \n",
    "\n",
    "custom_model  = Model(input, output, name= 'Custom_Model')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# compile model\n",
    "custom_model.compile(loss= 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']) \n",
    "\n",
    "# initialize callbacks \n",
    "reduceLR = ReduceLROnPlateau(monitor='val_loss', patience= 3, verbose= 1,                                  mode='min', factor=  0.2, min_lr = 1e-6) \n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience = 5 , verbose=1,                                  mode='min', restore_best_weights= True) \n",
    "\n",
    "checkpoint = ModelCheckpoint('CustomModel.weights.hdf5', monitor='val_loss',                               verbose=1,save_best_only=True, mode= 'min') \n",
    "\n",
    "callbacks= [reduceLR, early_stopping,checkpoint]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# define training config \n",
    "TRAIN_STEPS = 5177 // batch_size \n",
    "VAL_STEPS = 1722 //batch_size \n",
    "epochs = 80 \n",
    "\n",
    "# train model\n",
    "custom_model.fit(train_gen, steps_per_epoch= TRAIN_STEPS, validation_data=val_gen, validation_steps=VAL_STEPS, epochs= epochs, callbacks= callbacks)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Evaluate the model \n",
    "custom_model.evaluate(val_gen)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get validation labels\n",
    "val_labels = [] \n",
    "for i in range(VAL_STEPS + 1):\n",
    "  val_labels.extend(val_gen[i][1])\n",
    "\n",
    "val_labels = np.argmax(val_labels, axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# show classification report \n",
    "\n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "print(classification_report(val_labels, predicted_labels, target_names=classes))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# function to plot confusion matrix\n",
    "\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(actual, predicted):\n",
    "\n",
    "    cm = confusion_matrix(actual, predicted)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(7,7))\n",
    "    cmap=plt.cm.Blues\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title('Confusion matrix', fontsize=25)\n",
    "  \n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, fontsize=15)\n",
    "    plt.yticks(tick_marks, classes, fontsize=15)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], '.2f'),\n",
    "        horizontalalignment=\"center\",\n",
    "        color=\"white\" if cm[i, j] > thresh else \"black\", fontsize = 14)\n",
    "\n",
    "    plt.ylabel('True label', fontsize=20)\n",
    "    plt.xlabel('Predicted label', fontsize=20)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot confusion matrix\n",
    "plot_confusion_matrix(val_labels, predicted_labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initializing and fine-tuning the VGG16 model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import the VGG16 pretrained model \n",
    "from tensorflow.keras.applications import VGG16 \n",
    "\n",
    "# initialize the model vgg16 = VGG16(input_shape=(224,224,3), weights='imagenet', include_top=False) \n",
    "\n",
    "# Freeze all but the last 3 layers for layer in vgg16.layers[:-3]: layer.trainable = False \n",
    "\n",
    "# build model \n",
    "input = vgg16.layers[-1].output # input is the last output from vgg16 \n",
    "\n",
    "x = Dropout(0.25)(input) \n",
    "x = Flatten()(x) \n",
    "output = Dense(8, activation='softmax')(x) \n",
    "\n",
    "# create the model \n",
    "vgg16_model = Model(vgg16.input, output, name='VGG16_Model')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# compile the model \n",
    "vgg16_model.compile(optimizer= SGD(learning_rate=1e-3), loss= 'categorical_crossentropy', metrics= ['accuracy']) \n",
    "\n",
    "# reinitialize callbacks \n",
    "checkpoint = ModelCheckpoint('VggModel.weights.hdf5', monitor='val_loss', verbose=1,save_best_only=True, mode= 'min') \n",
    "\n",
    "callbacks= [reduceLR, early_stopping,checkpoint] \n",
    "\n",
    "# Train model \n",
    "vgg16_model.fit(train_gen, steps_per_epoch= TRAIN_STEPS, validation_data=val_gen, validation_steps=VAL_STEPS, epochs= epochs, callbacks= callbacks)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate VGG16"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Evaluate the model \n",
    "vgg16_model.evaluate(val_gen)\n",
    "\n",
    "# get the model predictions \n",
    "predicted_labels = np.argmax(vgg16_model.predict(val_gen), axis=1) \n",
    "\n",
    "# show classification report \n",
    "print(classification_report(val_labels, predicted_labels, target_names=classes)) \n",
    "\n",
    "# plot confusion matrix \n",
    "plot_confusion_matrix(val_labels, predicted_labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using a pretrained MobileNet model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# initializing the mobilenet model\n",
    "mobilenet = MobileNet(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
    "\n",
    "# freezing all but the last 5 layers\n",
    "for layer in mobilenet.layers[:-5]:\n",
    "  layer.trainable = False\n",
    "\n",
    "# add few mor layers\n",
    "x = mobilenet.layers[-1].output\n",
    "x = Dropout(0.5)(x)\n",
    "x = Flatten()(x) \n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(16, activation='relu')(x)\n",
    "output = Dense(8, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "mobilenet_model = Model(mobilenet.input, output, name= \"Mobilenet_Model\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# compile the model \n",
    "mobilenet_model.compile(optimizer= SGD(learning_rate=1e-3), loss= 'categorical_crossentropy', metrics= ['accuracy']) \n",
    "\n",
    "# reinitialize callbacks \n",
    "checkpoint = ModelCheckpoint('MobilenetModel.weights.hdf5', monitor='val_loss', verbose=1,save_best_only=True, mode= 'min') \n",
    "\n",
    "callbacks= [reduceLR, early_stopping,checkpoint] \n",
    "\n",
    "# model training \n",
    "mobilenet_model.fit(train_gen, steps_per_epoch= TRAIN_STEPS, validation_data=val_gen, validation_steps=VAL_STEPS, epochs=epochs, callbacks= callbacks)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Evaluate the model \n",
    "mobilenet_model.evaluate(val_gen) \n",
    "\n",
    "# get the model's predictions \n",
    "predicted_labels = np.argmax(mobilenet_model.predict(val_gen), axis=1) \n",
    "\n",
    "# show the classification report \n",
    "print(classification_report(val_labels, predicted_labels, target_names=classes)) \n",
    "\n",
    "# plot the confusion matrix \n",
    "plot_confusion_matrix(val_labels, predicted_labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Concatenation Ensemble"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# concatenate the models\n",
    "\n",
    "# import concatenate layer\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "# get list of models\n",
    "models = [custom_model, vgg16_model, mobilenet_model] \n",
    "\n",
    "input = Input(shape=(224, 224, 3), name='input') # input layer\n",
    "\n",
    "# get output for each model input\n",
    "outputs = [model(input) for model in models]\n",
    "\n",
    "# contenate the ouputs\n",
    "x = Concatenate()(outputs) \n",
    "\n",
    "# add further layers\n",
    "x = Dropout(0.5)(x) \n",
    "output = Dense(8, activation='softmax', name='output')(x) # output layer\n",
    "\n",
    "# create concatenated model\n",
    "conc_model = Model(input, output, name= 'Concatenated_Model')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# show model structure \n",
    "from tensorflow.keras.utils import plot_model \n",
    "plot_model(conc_model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Average Ensemble"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# average ensemble model \n",
    "\n",
    "# import Average layer\n",
    "from tensorflow.keras.layers import Average \n",
    "\n",
    "input = Input(shape=(224, 224, 3), name='input')  # input layer\n",
    "\n",
    "# get output for each input model\n",
    "outputs = [model(input) for model in models] \n",
    "\n",
    "# take average of the outputs\n",
    "x = Average()(outputs) \n",
    "\n",
    "x = Dense(16, activation='relu')(x) \n",
    "x = Dropout(0.3)(x) \n",
    "output = Dense(8, activation='softmax', name='output')(x) # output layer\n",
    "\n",
    "# create average ensembled model\n",
    "avg_model = Model(input, output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# show model structure \n",
    "plot_model(avg_model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Weighted Average Ensemble"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# function for setting weights\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def weight_init(shape =(1,1,3), weights=[1,2,3], dtype=tf.float32):\n",
    "    return tf.constant(np.array(weights).reshape(shape), dtype=dtype)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# implement custom weighted average layer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Concatenate\n",
    "\n",
    "class WeightedAverage(Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(WeightedAverage, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.W = self.add_weight(\n",
    "                    shape=(1,1,len(input_shape)),\n",
    "                    initializer=weighted_init,\n",
    "                    dtype=tf.float32,\n",
    "                    trainable=True)\n",
    "    def call(self, inputs):\n",
    "    \n",
    "        inputs = [tf.expand_dims(i, -1) for i in inputs]\n",
    "        inputs = Concatenate(axis=-1)(inputs) \n",
    "        weights = tf.nn.softmax(self.W, axis=-1)\n",
    "\n",
    "        return tf.reduce_mean(weights*inputs, axis=-1)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input = Input(shape=(224, 224, 3), name='input')  # input layer\n",
    "\n",
    "# get output for each input model\n",
    "outputs = [model(input) for model in models] \n",
    "\n",
    "# get weighted average of outputs\n",
    "x = WeightedAverage()(outputs)\n",
    "\n",
    "output = Dense(8, activation='softmax')(x) # output layer\n",
    "\n",
    "weighted_avg_model = Model(input, output, name= 'Weighted_AVerage_Model')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot model\n",
    "plot_model(weighted_avg_model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}